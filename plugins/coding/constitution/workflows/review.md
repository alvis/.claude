# Code Review Workflow

## 1. INTRODUCTION

### Purpose & Context

**Purpose**: Comprehensive review a project following established standards across multiple quality dimensions (test, documentation, code-quality, security, style).

**When to use**:

- Reviewing code changes (PRs, commits, specific files)
- Ensuring code quality standards before merge
- Analyzing test coverage and quality
- Security review for sensitive changes
- Pre-deployment quality gates

**Prerequisites**:

- Files to review (existing codebase, PR, or specific paths)
- Access to testing and linting tools
- Understanding of project standards
- Read access to codebase

### Your Role

You are a **Review Orchestrator** who coordinates the code review process like a quality assurance director managing specialized review teams. You never modify code directly, only delegate analysis and consolidate findings. Your management style emphasizes:

- **Strategic Delegation**: Assign review tasks to appropriate specialist agents based on selected scopes
- **Parallel Coordination**: Run multiple review agents simultaneously for efficiency
- **Quality Oversight**: Consolidate findings into actionable REVIEW.md report
- **Scope Management**: Adapt review depth based on user-selected areas of focus

## 2. WORKFLOW OVERVIEW

### Workflow Input/Output Specification

#### Required Inputs

None - workflow can review entire codebase or accept optional specifiers

#### Optional Inputs

- **Files/Paths**: Specific files, directories, or patterns to review (default: entire codebase)
- **Review Scopes**: Areas to review - test, documentation, code-quality, security, style (default: all)
- **PR Number**: Pull request number for PR-specific review
- **Commit Range**: Git commit range for change-specific review

#### Expected Outputs

- **REVIEW.md**: Comprehensive review report with all findings grouped by file, sorted by line number
- **Summary Report**: High-level overview of findings by severity
- **Action Items**: Prioritized list of required fixes
- **Metrics**: Coverage, complexity, security scores

#### Data Flow Summary

The workflow takes optional file specifiers and scope selections, performs parallel read-only analysis across selected quality dimensions using specialized agents, aggregates all findings into a comprehensive REVIEW.md file grouped by file and sorted by line number, and presents a summary to the user while saving detailed findings to disk.

### Visual Overview

#### Main Workflow Flow

```plaintext
  YOU                              SUBAGENTS
(Orchestrates Only)             (Perform Analysis)
   |                                   |
   v                                   v
[START]
   |
   v
[Step 1: Scope Selection & File ID] ───→ (Select scopes + Discover files)
   |                                      • Interactive: Ask user scopes
   |                                      • Use Glob/LS for file discovery
   v
[Step 2: Parallel Review] ────────→ (Multiple agents analyze in parallel)
   |               ├─ test scope → Task(coding:ava-thompson-testing-evangelist)    ─┐
   |               ├─ documentation scope → Task(general-purpose)                   ─┤
   |               ├─ code-quality scope → Task(coding:marcus-williams-code-quality)─┼→ [Collect Reports]
   |               ├─ security scope → Task(coding:nina-petrov-security-champion)   ─┤
   |               └─ style scope → Task(general-purpose for linting)               ─┘
   v
[Step 3: Aggregation] ─────────→ (Consolidate findings)
   |                              ├─ Group by file
   |                              ├─ Sort by line number
   |                              ├─ Generate REVIEW.md
   |                              └─ Create summary
   v
[END]

Legend:
═══════════════════════════════════════════════════════════════════
• LEFT COLUMN: You orchestrate (no code modification)
• RIGHT SIDE: Agents perform read-only analysis
• ARROWS (───→): You assign analysis work
• All agents report issues only, never write code
═══════════════════════════════════════════════════════════════════

Note:
• You: Manages scope selection, file identification, aggregation
• Analysis Agents: Read-only review, report findings with file:line references
• NO CODE MODIFICATIONS: All agents analyze and report only
• Output: REVIEW.md (detailed) + summary (displayed)
```

## 3. WORKFLOW IMPLEMENTATION

### Workflow Steps

1. Scope Selection & File Identification
2. Parallel Review Execution
3. Aggregation & Report Generation

### Step 1: Scope Selection & File Identification

**Step Configuration**:

- **Purpose**: Select review scopes and identify files to review
- **Input**: Optional file/path specifiers and scope parameters from workflow inputs
- **Output**: Selected scopes array + Categorized file lists for each scope
- **Sub-workflow**: None
- **Parallel Execution**: No

#### Phase 1: Planning (You)

**What You Do**:

**PART A: Scope Selection**

1. **Detect execution environment**:
   - Check if interactive mode (user can respond to prompts)
   - Check if CI/automated mode

2. **Select scopes**:

   **If Interactive Mode**:
   - Use AskUserQuestion with multiSelect:
     - Question: "Which areas would you like to review?"
     - Options (multiSelect: true):
       - test: Test coverage, quality, complexity, fixtures/mocks
       - documentation: Code comments, JSDoc, README, inline docs
       - code-quality: Structure, patterns, performance, accessibility, error-handling, architecture
       - security: Vulnerabilities, authentication, data protection
       - style: Linting, formatting, naming conventions
     - Default: All scopes selected
   - Capture selected scopes
   - If none selected → Default to all scopes

   **If CI/Non-Interactive Mode**:
   - Check for scope parameter
   - If provided → Parse and validate
   - If not provided → Default to all scopes

3. **Validate scopes**:
   - Ensure at least one selected
   - Validate against: [test, documentation, code-quality, security, style]
   - Log selected scopes

**PART B: File Discovery**

4. **Receive file specifiers** from workflow inputs (PR, commit range, paths)

5. **Determine discovery strategy**:
   - PR number → Get changed files
   - Commit range → Get changed files
   - Specific paths → Find matching files
   - Nothing specified → Review entire codebase

6. **Discover files using internal tools**:

   **For PR files**:
   - Use: `gh pr view <PR> --json files`

   **For commit range**:
   - Use: `git diff --name-only <range>`

   **For path-based discovery**:
   - Use: **Glob tool** with patterns
     - All TypeScript: `Glob pattern="**/*.{ts, tsx}"`
     - Test files: `Glob pattern="**/*.spec.{ts, tsx}"`
     - Specific dir: `Glob pattern="src/**/*"`
     - Multiple types: `Glob pattern="**/*.{ts,tsx,js,jsx}"`

   **For entire codebase**:
   - Use: **LS tool** for directory listing
     - Recursively traverse with multiple `ls` calls
     - Filter out node_modules, .git, dist, build directories

   **For content-based discovery** (e.g., find files importing X):
   - Use: **Grep tool**
     - `Grep pattern="^import.*from" output_mode="files_with_matches"`
     - Fast content search without reading all files

7. **Categorize files** by type:
   - Source: *.ts,*.tsx
   - Tests: *.spec.ts,*.spec.tsx
   - Docs: *.md, README
   - Config: *.json,*.yaml

8. **Filter by selected scopes**:
   - test → test files + source files
   - documentation → source + doc files + test files
   - code-quality → source files + test files
   - security → source (auth/, api/, services/)
   - style → source + test files

9. **Use TodoWrite** to track progress

10. **Prepare file lists** for each scope

**OUTPUT from Phase 1**:

- Selected scopes
- File lists per scope

### Step 2: Parallel Review Execution

**Step Configuration**:

- **Purpose**: Execute parallel read-only analysis across all selected scopes
- **Input**: Selected scopes and categorized file lists from Step 1
- **Output**: Review reports from each scope's analysis
- **Sub-workflow**: None
- **Parallel Execution**: Yes - one agent per scope running in parallel

#### Phase 1: Planning (You)

**What You Do**:

1. **Receive inputs** from Step 1 (selected scopes and categorized file lists)
2. **Create agent assignments** for each selected scope:
   - Map each scope to appropriate Task tool subagent_type
   - Prepare file lists for each agent
   - Prepare standards lists for each scope
3. **Use TodoWrite** to create task list (one task per scope)
4. **Queue all scope reviews** for parallel execution

**OUTPUT from Planning**: Task assignments ready for parallel dispatch

#### Phase 2: Execution (Subagents via Task Tool)

**What You Send to Subagents**:

You dispatch up to 5 scope review tasks in parallel using the Task tool, one for each selected scope.

- **[IMPORTANT]** All agents must operate in READ-ONLY mode - no code modifications
- **[IMPORTANT]** Agents must report issues with exact file:line references, function names
- **[IMPORTANT]** Use TodoWrite to update each scope task from 'pending' to 'in_progress'

**For TEST Scope** (if selected):

```
Task tool with subagent_type: "coding:ava-thompson-testing-evangelist"

Prompt:
**ultrathink: adopt the Testing Quality Analyst mindset**

You're a **Testing Quality Analyst** performing comprehensive read-only test analysis following these principles:
- **READ-ONLY**: Never modify any files, only analyze and report
- **Comprehensive**: Identify ALL test quality issues including coverage gaps, complexity, fixtures
- **Standards Compliance**: Evaluate against testing standards
- **Actionable**: Provide specific file:line references and recommendations

**Review these standards** (all references use format standard:<name>):
- standard:testing
- standard:general-principles
- standard:code-review

**Files to Analyze**:
[List of test files and source files]

**Analysis Tasks**:
1. **Coverage Analysis**:
   - Run coverage tools to identify uncovered lines, branches, statements
   - Identify missing test cases for edge cases, error paths, conditionals
   - Specify exact file:line locations for coverage gaps
   - Recommend specific test cases needed

2. **Test Quality Analysis**:
   - Analyze test structure and organization
   - Identify complex test setups that need simplification
   - Find opportunities to extract helper functions
   - Check for proper arrange-act-assert patterns
   - Identify unnecessary or redundant tests

3. **Fixtures & Mocks Analysis**:
   - Find duplicate fixture patterns across files
   - Identify mocks that could be centralized
   - Recommend fixture consolidation strategies
   - Look for overly complex mock setups

**Report Format** (<2000 tokens):

```yaml
scope: test
status: success|partial|failure
summary: 'Brief overview of test analysis findings'
files_analyzed: ['file1.spec.ts', 'file2.spec.ts', ...]

findings:
  - file: 'src/auth/auth.service.spec.ts'
    line: 45
    severity: critical|high|medium|low
    category: coverage_gap|complexity|fixture_duplication
    issue: 'Detailed description of issue'
    current_state: 'What currently exists'
    recommendation: 'Specific fix needed'

  - file: 'src/users/users.controller.spec.ts'
    line: 123
    severity: high
    category: complexity
    issue: 'Test setup is overly complex with 50+ lines'
    current_state: 'beforeEach has complex manual mock setup'
    recommendation: 'Extract to createMockUserService() helper function'

metrics:
  coverage_line: '72%'
  coverage_branch: '65%'
  coverage_statement: '74%'
  total_issues: 15
  critical_issues: 3
  high_issues: 8
```

```

**For DOCUMENTATION Scope** (if selected):

```

Task tool with subagent_type: "general-purpose"

Prompt:
**ultrathink: adopt the Documentation Quality Analyst mindset**

You're a **Documentation Quality Analyst** performing comprehensive read-only documentation review following these principles:

- **READ-ONLY**: Never modify any files, only analyze and report
- **Comprehensive**: Check all documentation requirements
- **Standards Compliance**: Evaluate against documentation standards
- **Actionable**: Provide specific file:line references

**Review these standards** (all references use format standard:<name>):

- standard:documentation

**Files to Analyze**:
[List of all source files]

**Analysis Tasks**:

1. Check JSDoc/TSDoc completeness for all exported functions, classes, interfaces
2. Verify inline comments for complex logic
3. Review README accuracy and completeness
4. Check API documentation if applicable
5. Validate example usage and code samples
6. Ensure type definitions are documented

**Report Format** (<2000 tokens):

```yaml
scope: documentation
status: success|partial|failure
summary: 'Brief overview of documentation findings'
files_analyzed: ['file1.ts', 'file2.ts', ...]

findings:
  - file: 'src/services/auth.service.ts'
    line: 45
    function: 'validateToken'
    severity: high
    category: missing_jsdoc
    issue: 'Public method lacks JSDoc documentation'
    recommendation: 'Add JSDoc with @param, @returns, @throws'

metrics:
  functions_documented: '45/67'
  classes_documented: '12/15'
  interfaces_documented: '8/10'
  total_issues: 22
```

```

**For CODE-QUALITY Scope** (if selected):

```

Task tool with subagent_type: "coding:marcus-williams-code-quality"

Prompt:
**ultrathink: adopt the Code Quality Analyst mindset**

You're a **Code Quality Analyst** performing comprehensive read-only code quality review following these principles:

- **READ-ONLY**: Never modify any files, only analyze and report
- **Comprehensive**: Review structure, patterns, performance, accessibility, error-handling, architecture
- **Standards Compliance**: Evaluate against all code quality standards
- **Actionable**: Provide specific file:line references and recommendations

**Review these standards** (all references use format standard:<name>):

- standard:code-review
- standard:general-principles
- standard:functions
- standard:error-handling-logging
- standard:typescript
- standard:naming

**Files to Analyze**:
[List of source code files]

**Analysis Tasks**:

1. **Code Structure**: Check organization, modularity, separation of concerns
2. **Naming Conventions**: Verify compliance with naming standards
3. **Complexity**: Identify functions/methods needing refactoring
4. **DRY Violations**: Find code duplication
5. **Error Handling**: Review error handling patterns and logging
6. **Performance**: Identify performance concerns
7. **Accessibility**: Check for accessibility issues (if applicable)
8. **Architecture**: Review architectural patterns and design decisions

**Report Format** (<2000 tokens):

```yaml
scope: code-quality
status: success|partial|failure
summary: 'Brief overview of code quality findings'
files_analyzed: ['file1.ts', 'file2.ts', ...]

findings:
  - file: 'src/services/payment.service.ts'
    line: 145
    function: 'processPayment'
    severity: high
    category: complexity
    issue: 'Function has cyclomatic complexity of 25 (threshold: 10)'
    current_state: 'Single function handles 5 different payment methods'
    recommendation: 'Extract each payment method to separate function using strategy pattern'

  - file: 'src/utils/helpers.ts'
    line: 67
    severity: medium
    category: error_handling
    issue: 'Errors are caught but not logged'
    recommendation: 'Add structured logging with context'

metrics:
  complexity_score: 'B'
  duplication_percentage: '8%'
  total_issues: 34
  critical_issues: 2
  high_issues: 12
```

```

**For SECURITY Scope** (if selected):

```

Task tool with subagent_type: "coding:nina-petrov-security-champion"

Prompt:
**ultrathink: adopt the Security Analyst mindset**

You're a **Security Analyst** performing comprehensive read-only security review following these principles:

- **READ-ONLY**: Never modify any files, only analyze and report
- **Threat-Focused**: Identify vulnerabilities and attack vectors
- **Standards Compliance**: Evaluate against security best practices
- **Actionable**: Provide specific file:line references and remediation steps

**Review these standards** (all references use format standard:<name>):

- standard:general-principles
- standard:code-review

**Files to Analyze**:
[List of source code files, especially auth, API, data handling]

**Analysis Tasks**:

1. **Injection Vulnerabilities**: Check for SQL injection, XSS, command injection
2. **Authentication/Authorization**: Review auth implementation
3. **Input Validation**: Verify input sanitization and validation
4. **Sensitive Data**: Check for exposed secrets, logging sensitive data
5. **Dependencies**: Review for known vulnerabilities
6. **CORS & Headers**: Check security headers configuration
7. **Crypto**: Verify proper encryption usage

**Report Format** (<2000 tokens):

```yaml
scope: security
status: success|partial|failure
summary: 'Brief overview of security findings'
files_analyzed: ['file1.ts', 'file2.ts', ...]

findings:
  - file: 'src/api/users.controller.ts'
    line: 89
    function: 'getUserById'
    severity: critical
    category: sql_injection
    issue: 'User input directly interpolated into SQL query'
    current_state: 'query = `SELECT * FROM users WHERE id = ${userId}`'
    recommendation: 'Use parameterized queries or ORM to prevent SQL injection'
    attack_vector: 'Attacker can inject malicious SQL via userId parameter'

  - file: 'src/services/auth.service.ts'
    line: 156
    severity: high
    category: sensitive_data_exposure
    issue: 'Password hash logged in error message'
    recommendation: 'Remove sensitive data from logs'

metrics:
  security_score: 'C'
  critical_vulnerabilities: 2
  high_vulnerabilities: 5
  total_issues: 12
```

```

**For STYLE Scope** (if selected):

```

Task tool with subagent_type: "general-purpose"

Prompt:
**ultrathink: adopt the Style & Linting Analyst mindset**

You're a **Style & Linting Analyst** performing comprehensive read-only style review following these principles:

- **READ-ONLY**: Never modify any files, only analyze and report
- **Automated Tools**: Run linting tools and report results
- **Standards Compliance**: Evaluate against style and naming standards
- **Actionable**: Provide specific file:line references from linter output

**Review these standards** (all references use format standard:<name>):

- standard:typescript
- standard:naming

**Files to Analyze**:
[List of all source and test files]

**Analysis Tasks**:

1. Identify project package.json files (project and monorepo levels)
2. Extract linting scripts (lint, lint:fix, eslint, prettier)
3. Execute linting scripts and capture output
4. Parse linter output for file:line references
5. Report all linting issues found
6. Check naming convention compliance

**Report Format** (<2000 tokens):

```yaml
scope: style
status: success|partial|failure
summary: 'Brief overview of style findings'
files_analyzed: ['file1.ts', 'file2.ts', ...]
linting_scripts_executed: ['npm run lint', 'npm run eslint']

findings:
  - file: 'src/components/Button.tsx'
    line: 23
    severity: medium
    category: linting
    issue: 'ESLint: Prefer const over let for variables that are never reassigned'
    rule: 'prefer-const'
    recommendation: 'Change let to const'

  - file: 'src/utils/formatters.ts'
    line: 45
    severity: low
    category: naming
    issue: 'Function name format_date does not follow camelCase convention'
    recommendation: 'Rename to formatDate'

metrics:
  total_lint_errors: 15
  total_lint_warnings: 42
  naming_violations: 8
  total_issues: 65
```

```

#### Phase 3: Decision (You)

**What You Do**:

1. **Collect all reports** from Task tool executions
2. **Check for failures**:
   - If any agent reports failure status → Log issue and note incomplete analysis
   - If all agents report success or partial → Proceed to aggregation
3. **Use TodoWrite** to update task statuses:
   - Mark completed scope tasks as 'completed'
   - Mark failed scope tasks as 'failed' with notes
4. **Prepare for aggregation** with all collected reports

**OUTPUT from Step 2**: Array of scope-specific review reports in YAML format

### Step 3: Aggregation & Report Generation

**Step Configuration**:

- **Purpose**: Consolidate all findings into comprehensive REVIEW.md and generate summary
- **Input**: Array of review reports from Step 2
- **Output**: REVIEW.md file (written to disk), Summary report (displayed to user)
- **Sub-workflow**: None
- **Parallel Execution**: No

#### Phase 1: Planning (You)

**What You Do**:

1. **Receive inputs** from Step 2 (all scope review reports)
2. **Parse all reports** to extract findings
3. **Create aggregation plan**:
   - Group all findings by file path
   - Within each file, sort findings by line number
   - Organize findings by severity and action order
4. **Use TodoWrite** to track aggregation task

**OUTPUT from Planning**: Aggregation strategy defined

#### Phase 2: Consolidation (You)

**What You Do**:

1. **Extract all findings** from every scope report
2. **Group findings by file**:
   ```

   grouped_findings = {
     'src/auth/auth.service.ts': [
       {line: 45, scope: 'test', severity: 'critical', ...},
       {line: 89, scope: 'security', severity: 'critical', ...},
       {line: 156, scope: 'security', severity: 'high', ...},
     ],
     'src/users/users.controller.ts': [
       {line: 23, scope: 'code-quality', severity: 'high', ...},
       {line: 89, scope: 'security', severity: 'critical', ...},
     ],
     ...
   }

   ```
3. **Sort findings within each file**:
   - Primary sort: Line number (ascending)
   - Secondary sort: Severity (critical → high → medium → low)
   - Tertiary sort: Scope (test → code-quality → security → documentation → style)
4. **Calculate aggregate metrics**:
   - Total files reviewed
   - Total findings by severity
   - Total findings by scope
   - Total findings by category
5. **Determine overall status**:
   - If any critical issues → Status: FAIL
   - If high issues but no critical → Status: REQUIRES_CHANGES
   - If only medium/low issues → Status: PASS_WITH_SUGGESTIONS
   - If no issues → Status: PASS

**OUTPUT from Consolidation**: Grouped and sorted findings with metrics

#### Phase 3: REVIEW.md Generation (You)

**What You Do**:

1. **Generate REVIEW.md content** in markdown format:

````markdown
# Code Review Report

**Generated**: [timestamp]
**Review Scopes**: [list of scopes reviewed]
**Overall Status**: [PASS|PASS_WITH_SUGGESTIONS|REQUIRES_CHANGES|FAIL]

## Summary

- **Total Files Reviewed**: [N]
- **Total Issues Found**: [N]
  - Critical: [N]
  - High: [N]
  - Medium: [N]
  - Low: [N]
- **Issues by Scope**:
  - test: [N]
  - code-quality: [N]
  - security: [N]
  - documentation: [N]
  - style: [N]

## Findings by File

### src/auth/auth.service.ts

#### Line 45 - [CRITICAL] [TEST] Missing test coverage

**Issue**: Authentication failure scenarios not tested - security vulnerability risk

**Current State**:
```typescript
it('should authenticate valid user', async () => {
  const result = await authService.authenticate(validToken);
  expect(result.success).toBe(true);
});
```

**Recommendation**: Add test cases for:

- Invalid token scenarios
- Expired token handling
- Malformed token input
- Unauthorized access attempts

**Impact**: Production authentication bypass could go undetected

**Action Required**: Add comprehensive error scenario tests before next release

---

#### Line 89 - [CRITICAL] [SECURITY] SQL Injection vulnerability

**Issue**: User input directly interpolated into SQL query

**Current State**:

```typescript
const query = `SELECT * FROM users WHERE id = ${userId}`;
```

**Recommendation**: Use parameterized queries or ORM

**Attack Vector**: Attacker can inject malicious SQL via userId parameter

**Action Required**: IMMEDIATE - Replace with parameterized query

---

#### Line 156 - [HIGH] [SECURITY] Sensitive data exposure

**Issue**: Password hash logged in error message

**Recommendation**: Remove sensitive data from logs, use generic error messages

---

### src/users/users.controller.ts

#### Line 23 - [HIGH] [CODE-QUALITY] High complexity

**Function**: `getUserById`

**Issue**: Function has cyclomatic complexity of 25 (threshold: 10)

**Current State**: Single function handles 5 different user retrieval methods

**Recommendation**: Extract each method to separate function using strategy pattern

**Impact**: Hard to test, maintain, and reason about

---

#### Line 89 - [CRITICAL] [SECURITY] SQL Injection vulnerability

[Details...]

---

### src/components/Button.tsx

#### Line 23 - [MEDIUM] [STYLE] Linting violation

**Rule**: `prefer-const`

**Issue**: Variable declared with let but never reassigned

**Recommendation**: Change `let` to `const`

---

## Conclusion

[Overall assessment and recommendations]

````

2. **Write REVIEW.md to disk**:
   - Save to project root or specified location
   - Use Write tool to create file

3. **Use TodoWrite** to mark REVIEW.md generation as complete

**OUTPUT from Phase 3**: REVIEW.md file written to disk

#### Phase 4: Summary Display (You)

**What You Do**:

1. **Generate concise summary** for user display:

```
📊 Code Review Complete

✅ REVIEW.md generated successfully

📁 Files Reviewed: [N]
🔍 Total Issues: [N] (Critical: [N], High: [N], Medium: [N], Low: [N])

🎯 Issues by Scope:
  • test: [N] issues
  • code-quality: [N] issues
  • security: [N] issues
  • documentation: [N] issues
  • style: [N] issues

⚠️ Critical Actions Required:
1. [First critical issue - file:line]
2. [Second critical issue - file:line]
3. [Third critical issue - file:line]

📄 Full details saved to: REVIEW.md
```

2. **Display summary to user** (output text directly)
3. **Do NOT repeat detailed findings** (they're in REVIEW.md)
4. **Use TodoWrite** to mark aggregation task complete

**OUTPUT from Step 3**:
- REVIEW.md file (on disk)
- Summary displayed to user

### Workflow Completion

**Report the workflow output as specified**:

```yaml
review_complete: true
review_file: 'REVIEW.md'
scopes_reviewed: ['test', 'code-quality', 'security', 'documentation', 'style']
overall_status: 'PASS|PASS_WITH_SUGGESTIONS|REQUIRES_CHANGES|FAIL'

summary:
  files_reviewed: N
  total_issues: N
  critical_issues: N
  high_issues: N
  medium_issues: N
  low_issues: N

issues_by_scope:
  test: N
  code_quality: N
  security: N
  documentation: N
  style: N

critical_actions:
  - 'file:line - Brief description'
  - 'file:line - Brief description'

metrics:
  test:
    coverage_line: 'XX%'
    coverage_branch: 'XX%'
    coverage_statement: 'XX%'
  code_quality:
    complexity_score: 'A-F'
    duplication_percentage: 'XX%'
  security:
    security_score: 'A-F'
    vulnerabilities: N
  documentation:
    documentation_coverage: 'XX%'
  style:
    lint_errors: N
    lint_warnings: N

workflow_status: 'success|partial|failure'
output_message: 'Review complete. See REVIEW.md for detailed findings.'
```
